{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pendahuluan \u00b6 Nama: Aisy Qonitah Suwardi NIM: 160411100015 Mata Kuliah: Penambangan dan Pencarian Web Apa Itu Web Crawler? \u00b6 Pengertian \u00b6 Web Crawler adalah suatu program atau script otomat yang relatif simple, yang dengan metode tertentu melakukan scan atau \u201ccrawl\u201d ke semua halaman-halaman Internet untuk membuat index dari data yang dicarinya. Nama lain untuk web crawl adalah web spider, web robot, bot, crawl dan automatic indexer. Fungsi \u00b6 Web Crawl dapat digunakan untuk beragam tujuan. Penggunaan yang paling umum adalah yang terkait dengan search engine. Search engine menggunakan web crawl untuk mengumpulkan informasi mengenai apa yang ada di halaman-halaman web publik. Tujuan utamanya adalah mengumpukan data sehingga ketika pengguna Internet mengetikkan kata pencarian di komputernya, search engine dapat dengan segera menampilkan web site yang relevan. Cara Kerja \u00b6 Mesin pencari web bekerja dengan cara menyimpan informasi tentang banyak halaman web, yang diambil langsung dari WWW. Halaman-halaman ini diambil dengan web crawler \u2014 browser web otomatis yang mengikuti setiap pranala yang dilihatnya. Isi setiap halaman lalu dianalisis untuk menentukan cara mengindeksnya (misalnya, kata-kata diambil dari judul, subjudul, atau field khusus yang disebut meta tag). Data tentang halaman web disimpan dalam sebuah database indeks untuk digunakan dalam pencarian selanjutnya. Mesin pencari juga menyimpan dan memberikan informasi hasil pencarian berupa pranala yang merujuk pada file, seperti file audio, file video, gambar, foto dan sebagainya. Ketika seorang pengguna mengunjungi mesin pencari dan memasukkan query, biasanya dengan memasukkan kata kunci, mesin mencari indeks dan memberikan daftar halaman web yang paling sesuai dengan kriterianya.","title":"Home"},{"location":"#pendahuluan","text":"Nama: Aisy Qonitah Suwardi NIM: 160411100015 Mata Kuliah: Penambangan dan Pencarian Web","title":"Pendahuluan"},{"location":"#apa-itu-web-crawler","text":"","title":"Apa Itu Web Crawler?"},{"location":"#pengertian","text":"Web Crawler adalah suatu program atau script otomat yang relatif simple, yang dengan metode tertentu melakukan scan atau \u201ccrawl\u201d ke semua halaman-halaman Internet untuk membuat index dari data yang dicarinya. Nama lain untuk web crawl adalah web spider, web robot, bot, crawl dan automatic indexer.","title":"Pengertian"},{"location":"#fungsi","text":"Web Crawl dapat digunakan untuk beragam tujuan. Penggunaan yang paling umum adalah yang terkait dengan search engine. Search engine menggunakan web crawl untuk mengumpulkan informasi mengenai apa yang ada di halaman-halaman web publik. Tujuan utamanya adalah mengumpukan data sehingga ketika pengguna Internet mengetikkan kata pencarian di komputernya, search engine dapat dengan segera menampilkan web site yang relevan.","title":"Fungsi"},{"location":"#cara-kerja","text":"Mesin pencari web bekerja dengan cara menyimpan informasi tentang banyak halaman web, yang diambil langsung dari WWW. Halaman-halaman ini diambil dengan web crawler \u2014 browser web otomatis yang mengikuti setiap pranala yang dilihatnya. Isi setiap halaman lalu dianalisis untuk menentukan cara mengindeksnya (misalnya, kata-kata diambil dari judul, subjudul, atau field khusus yang disebut meta tag). Data tentang halaman web disimpan dalam sebuah database indeks untuk digunakan dalam pencarian selanjutnya. Mesin pencari juga menyimpan dan memberikan informasi hasil pencarian berupa pranala yang merujuk pada file, seperti file audio, file video, gambar, foto dan sebagainya. Ketika seorang pengguna mengunjungi mesin pencari dan memasukkan query, biasanya dengan memasukkan kata kunci, mesin mencari indeks dan memberikan daftar halaman web yang paling sesuai dengan kriterianya.","title":"Cara Kerja"},{"location":"cluster/","text":"Clustering \u00b6 Clustering adalah jenis pembelajaran tanpa pengawasan . Ini sangat sering digunakan ketika Anda tidak memiliki label data. K-Means Clustering adalah salah satu algoritma pengelompokan populer. Tujuan dari algoritma ini adalah untuk menemukan grup (cluster) dalam data yang diberikan. K-Means \u00b6 K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Library yang digunakan yaitu scikit-learn,seperti dibawah ini from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(tfidf_matrix, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) Disini kita membagi cluster sebanyak 5 bagian. Jumlah cluster dapat diubah sesuai kebutuhan. Cara membaginya: kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) Silhouette Coeffisien \u00b6 Silhouette Coefficient merupakan metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut ai. Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut bi. Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : \u200b Dibawah ini merupakan code yang digunakan untuk menghitung jarak rata-rata antara cluster dan titik dan juga menghitung jarak terdekat cluster s_avg = silhouette_score(tfidf_matrix, kmeans.labels_, random_state=10) print(s_avg)","title":"Clustering"},{"location":"cluster/#clustering","text":"Clustering adalah jenis pembelajaran tanpa pengawasan . Ini sangat sering digunakan ketika Anda tidak memiliki label data. K-Means Clustering adalah salah satu algoritma pengelompokan populer. Tujuan dari algoritma ini adalah untuk menemukan grup (cluster) dalam data yang diberikan.","title":"Clustering"},{"location":"cluster/#k-means","text":"K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Library yang digunakan yaitu scikit-learn,seperti dibawah ini from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(tfidf_matrix, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) Disini kita membagi cluster sebanyak 5 bagian. Jumlah cluster dapat diubah sesuai kebutuhan. Cara membaginya: kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru)","title":"K-Means"},{"location":"cluster/#silhouette-coeffisien","text":"Silhouette Coefficient merupakan metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut ai. Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut bi. Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : \u200b Dibawah ini merupakan code yang digunakan untuk menghitung jarak rata-rata antara cluster dan titik dan juga menghitung jarak terdekat cluster s_avg = silhouette_score(tfidf_matrix, kmeans.labels_, random_state=10) print(s_avg)","title":"Silhouette Coeffisien"},{"location":"import/","text":"Import Data ke CSV \u00b6 def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Disini kita membuat fungsi write csv agar fungsinya dapat digunakan berulang-ulang dan juga data yang kita crawl dapat disimpan dalam bentuk csv agar lebih memudkan jika ingin diakses.","title":"Import Data ke CSV"},{"location":"import/#import-data-ke-csv","text":"def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Disini kita membuat fungsi write csv agar fungsinya dapat digunakan berulang-ulang dan juga data yang kita crawl dapat disimpan dalam bentuk csv agar lebih memudkan jika ingin diakses.","title":"Import Data ke CSV"},{"location":"library/","text":"Install Library \u00b6 Requests \u00b6 Requests merupakan modul Python yang bisa kamu gunakan untuk mengirim berbagai request HTTP. Requests adalah library yang mudah digunakan dengan banyak fitur mulai dari melempar parameter dalam URL sampai mengirim header khusus dan verifikasi SSL. Cara Install: pip install requests Setelah kamu menginstal modul tersebut, kamu bisa memastikan instalasi sukses dengan mengimportnya dengan menggunakan perintah ini: import requests Jika instalasi berhasil, kamu tidak akan melihat pesan error apapun. BeautifulSoup4 \u00b6 Beautiful Soup 4 merupakan sebuah library Python yang memungkinkan kita untuk melakukan scraping dengan mudah dan cepat Cara Install: pip install beautifulsoup4 pip install bs4 Numpy \u00b6 Numpy memiliki kegunaan untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. Numpy merupakan salah satu library yang digunakan oleh library lain seperti Scikit-Learn untuk keperluan analisis data. Cara Install: pip install numpy Sastrawi \u00b6 Sastrawi perpustakaan PHP sederhana yang memungkinkan Anda untuk mengurangi kata-kata yang terinfleksi dalam Bahasa Indonesia (Bahasa Indonesia) ke bentuk dasarnya (batang) Cara Install: pip install Sastrawi Scipy \u00b6 Scipy digunakan untuk menangani operasi aljabar dan matriks serta operasi matematika lainya. Disini kamu dapat menangani sejumlah operasi matematika yang lebih kompleks daripada menggunakan library math bawaan Python. Cara Install: pip install spicy Scikit-learn \u00b6 Scikit-learn merupakan suatu tools atau library yang handal dan efisien untuk data mining maupun data analisis. untuk menginstall scikit-learn harus lebih dulu menginstall Numpy dan Scipy Cara Install: pip install scikit-learn Sqlite3 \u00b6 Sqlite3 merupakan pustaka C yang menyediakan basis data berbasis disk yang ringan yang tidak memerlukan proses server terpisah dan memungkinkan mengakses basis data menggunakan varian tidak standar dari bahasa query SQL. Beberapa aplikasi dapat menggunakan SQLite untuk penyimpanan data internal. Modul sqlite3 merupakan bawaan dari python. CSV \u00b6 CSV atau comma separated value adalah salah satu tipe file yang digunakan secara luas di dunia programming . Tidak hanya itu CSV pun sering digunakan dalam pengolahan informasi yang dihasilkan spreadsheet untuk diproses lebih lanjut melalui mesin analitik. CSV pun dianggap sebagai file yang agnostik karena dapat digunakan oleh berbagai database untuk proses backup data . Modul CSV merupakan bawaan dari python.","title":"Library"},{"location":"library/#install-library","text":"","title":"Install Library"},{"location":"library/#requests","text":"Requests merupakan modul Python yang bisa kamu gunakan untuk mengirim berbagai request HTTP. Requests adalah library yang mudah digunakan dengan banyak fitur mulai dari melempar parameter dalam URL sampai mengirim header khusus dan verifikasi SSL. Cara Install: pip install requests Setelah kamu menginstal modul tersebut, kamu bisa memastikan instalasi sukses dengan mengimportnya dengan menggunakan perintah ini: import requests Jika instalasi berhasil, kamu tidak akan melihat pesan error apapun.","title":"Requests"},{"location":"library/#beautifulsoup4","text":"Beautiful Soup 4 merupakan sebuah library Python yang memungkinkan kita untuk melakukan scraping dengan mudah dan cepat Cara Install: pip install beautifulsoup4 pip install bs4","title":"BeautifulSoup4"},{"location":"library/#numpy","text":"Numpy memiliki kegunaan untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. Numpy merupakan salah satu library yang digunakan oleh library lain seperti Scikit-Learn untuk keperluan analisis data. Cara Install: pip install numpy","title":"Numpy"},{"location":"library/#sastrawi","text":"Sastrawi perpustakaan PHP sederhana yang memungkinkan Anda untuk mengurangi kata-kata yang terinfleksi dalam Bahasa Indonesia (Bahasa Indonesia) ke bentuk dasarnya (batang) Cara Install: pip install Sastrawi","title":"Sastrawi"},{"location":"library/#scipy","text":"Scipy digunakan untuk menangani operasi aljabar dan matriks serta operasi matematika lainya. Disini kamu dapat menangani sejumlah operasi matematika yang lebih kompleks daripada menggunakan library math bawaan Python. Cara Install: pip install spicy","title":"Scipy"},{"location":"library/#scikit-learn","text":"Scikit-learn merupakan suatu tools atau library yang handal dan efisien untuk data mining maupun data analisis. untuk menginstall scikit-learn harus lebih dulu menginstall Numpy dan Scipy Cara Install: pip install scikit-learn","title":"Scikit-learn"},{"location":"library/#sqlite3","text":"Sqlite3 merupakan pustaka C yang menyediakan basis data berbasis disk yang ringan yang tidak memerlukan proses server terpisah dan memungkinkan mengakses basis data menggunakan varian tidak standar dari bahasa query SQL. Beberapa aplikasi dapat menggunakan SQLite untuk penyimpanan data internal. Modul sqlite3 merupakan bawaan dari python.","title":"Sqlite3"},{"location":"library/#csv","text":"CSV atau comma separated value adalah salah satu tipe file yang digunakan secara luas di dunia programming . Tidak hanya itu CSV pun sering digunakan dalam pengolahan informasi yang dihasilkan spreadsheet untuk diproses lebih lanjut melalui mesin analitik. CSV pun dianggap sebagai file yang agnostik karena dapat digunakan oleh berbagai database untuk proses backup data . Modul CSV merupakan bawaan dari python.","title":"CSV"},{"location":"preprosesing/","text":"Preprocessing \u00b6 Pengertian \u00b6 Text Preprocessing adalah tahapan dimana kita melakukan seleksi data agar data yang akan kita olah menjadi lebih terstruktur. Tahap Text Preprocessing adalah tahapan dimana aplikasi melakukan seleksi data yang akan diproses pada setiap dokumen. Proses preprocessing ini meliputi : Case Folding yaitu mengkonversi keseluruhan teks dalam dokumen menjadi suatu bentuk standar (biasanya huruf kecil atau lowercase). Tokenisasi yaitu pemotongan string input berdasarkan tiap kata yang menyusunnya. Filtering yaitu tahap mengambil kata-kata penting dari hasil token. Bisa menggunakan algoritma stoplist (membuang kata kurang penting) atau wordlist (menyimpan kata penting). Stemming yaitu memperkecil jumlah indeks yang berbeda dari suatu dokumen, juga untuk melakukan pengelompokan kata-kata lain yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapatkan imbuhan yang berbeda. Tokenisasi \u00b6 Digunakan untuk memecah suatu kalimat menjadi beberapa kata. Jika ada kalimat \"nama saya adalah sisi\" ingin dipecah menjadi 2 suku kata maka hasilnya \"nama saya\" \"adalah aisy\" dan jika ingin dipecah menjadi 1 suku kata akan menjadi \"nama\" , \"saya\" , \"adalah\" , \"aisy\". Ini merupakan code yang dapat digunakan jika menggunakan tokenisasi def tokenisasi (txt,ngram=1): token=[] start=0 end=ngram txtSplit=txt.split() while end <=len (txtSplit): tmp=txtSplit[start:end] frase='' for i in tmp: frase += i+' ' token.append(frase) end+=1;start+=1; return token Filtering dan Stemming \u00b6 Code dibawah ini digunakan untuk membuang kata yang tidak penting dan mengelompokan kata-kata yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapatkan imbuhan yang berbeda. def preprosesing(txt): SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() hasil = '' for i in txt.split(): if i[:-2].isalpha(): stop = stopword.remove(i) stem = stemmer.stem(stop) hasil += stem + ' ' return hasil SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() Code diatas digunakan memanggil StopWordRemoverFactory yang ada pada library Sastrawi untuk membuang kata yang tidak penting yang sudah didefinisakan di library Sastrawi. Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() Code diatas digunakan untuk mengambil StemmerFactory yang ada pada library Sastrawi yang digunakan untuk mengambil kata dasar yang sudah didefinisikan pada library Sastrawi. for i in txt.split(): if i[:-2].isalpha(): stop = stopword.remove(i) stem = stemmer.stem(stop) hasil += stem + ' ' return hasil Code diatas digunakan untuk nembuang semua karakter selain alfabet.","title":"Preprosesing"},{"location":"preprosesing/#preprocessing","text":"","title":"Preprocessing"},{"location":"preprosesing/#pengertian","text":"Text Preprocessing adalah tahapan dimana kita melakukan seleksi data agar data yang akan kita olah menjadi lebih terstruktur. Tahap Text Preprocessing adalah tahapan dimana aplikasi melakukan seleksi data yang akan diproses pada setiap dokumen. Proses preprocessing ini meliputi : Case Folding yaitu mengkonversi keseluruhan teks dalam dokumen menjadi suatu bentuk standar (biasanya huruf kecil atau lowercase). Tokenisasi yaitu pemotongan string input berdasarkan tiap kata yang menyusunnya. Filtering yaitu tahap mengambil kata-kata penting dari hasil token. Bisa menggunakan algoritma stoplist (membuang kata kurang penting) atau wordlist (menyimpan kata penting). Stemming yaitu memperkecil jumlah indeks yang berbeda dari suatu dokumen, juga untuk melakukan pengelompokan kata-kata lain yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapatkan imbuhan yang berbeda.","title":"Pengertian"},{"location":"preprosesing/#tokenisasi","text":"Digunakan untuk memecah suatu kalimat menjadi beberapa kata. Jika ada kalimat \"nama saya adalah sisi\" ingin dipecah menjadi 2 suku kata maka hasilnya \"nama saya\" \"adalah aisy\" dan jika ingin dipecah menjadi 1 suku kata akan menjadi \"nama\" , \"saya\" , \"adalah\" , \"aisy\". Ini merupakan code yang dapat digunakan jika menggunakan tokenisasi def tokenisasi (txt,ngram=1): token=[] start=0 end=ngram txtSplit=txt.split() while end <=len (txtSplit): tmp=txtSplit[start:end] frase='' for i in tmp: frase += i+' ' token.append(frase) end+=1;start+=1; return token","title":"Tokenisasi"},{"location":"preprosesing/#filtering-dan-stemming","text":"Code dibawah ini digunakan untuk membuang kata yang tidak penting dan mengelompokan kata-kata yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapatkan imbuhan yang berbeda. def preprosesing(txt): SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() hasil = '' for i in txt.split(): if i[:-2].isalpha(): stop = stopword.remove(i) stem = stemmer.stem(stop) hasil += stem + ' ' return hasil SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() Code diatas digunakan memanggil StopWordRemoverFactory yang ada pada library Sastrawi untuk membuang kata yang tidak penting yang sudah didefinisakan di library Sastrawi. Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() Code diatas digunakan untuk mengambil StemmerFactory yang ada pada library Sastrawi yang digunakan untuk mengambil kata dasar yang sudah didefinisikan pada library Sastrawi. for i in txt.split(): if i[:-2].isalpha(): stop = stopword.remove(i) stem = stemmer.stem(stop) hasil += stem + ' ' return hasil Code diatas digunakan untuk nembuang semua karakter selain alfabet.","title":"Filtering dan Stemming"},{"location":"proses/","text":"Proses Web Crawling \u00b6 Halaman Web \u00b6 Untuk website yang akan di crawl datanya yaitu: https://palominobag.com/best-seller Data yang ada pada website tersebut sebanyak 153 data. Proses Crawling \u00b6 Untuk mengakses halaman web dan tag html digunakan library: import requests from bs4 import BeautifulSoup Untuk menghubungkan ke website dan mengambil tag html digunakan code berikut: page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') links = soup.findAll(class_='product-name') links = soup.findAll(class_='product-name') Code diatas digunakan untuk mendapatkan semua tag html pada halaman web tersebut. Dan hasinya berupa list. Menentukan informasi apa saja yang akan diambil pada halaman web. Kemudian kita mengambil data berdasarkan class pada code html di website tersebut. Disini kita mengambil data yang berada pada class='product-name': for i in links: try : url = i.find ('a')['href'] print(url) page = requests.get(url) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find(class_='grid_12 alpha omega') judul = konten.find(class_='product-name no-rel').getText() harga = konten.find(class_='price').getText() detil = konten.find(class_='box-collateral').getText(' ') Untuk mengambil url yang ada pada class='product-name' digunakan: url = i.find ('a')['href'] print(url) Untuk mengambil data judul, harga, dan detil barang yang berada class='product-name' dan kemudian pencarian dipersempit lagi di class='grid_12 alpha omega' menggunakan: konten = soup.find(class_='grid_12 alpha omega') judul = konten.find(class_='product-name no-rel').getText() harga = konten.find(class_='price').getText() detil = konten.find(class_='box-collateral').getText(' ') Variable konten diatas menyimpan setiap data yang diambil pada class='grid_12 alpha omega' kemudian variable judul, harga,detil akan mengambil data yang telah ditampung didalam variable konten Untuk mengambil sebanyak 153 data yang ada pada website tesebut menggunakan: url=[str(i) for i in range(0,5)] for a in url: src= ('https://palominobag.com/best-seller?limit=36&p='+a) crawl (src) conn.commit() Membuat dan Menyimpan Data di Database \u00b6 Untuk membuat database dan tablenya menggunakan code dibawah ini: conn = sqlite3.connect('ba.db') c = 1 choice = input(\"Update data? Y/N\").lower() if choice == 'y': conn.execute('drop table if exists BOOK') conn.execute('''CREATE TABLE BOOK (JUDUL TEXT NOT NULL, HARGA TEXT NOT NULL, DESKRIPSI TEXT NOT NULL);''') Kemudian data yang telah di crawl diatas disimpan kedalam table database yang telah dibuat menggunakan code dibawah ini: conn.execute(\"INSERT INTO BOOK \\ VALUES (?, ?, ?)\", (judul, harga, detil)); except AttributeError: print(\"\") conn.commit() except ValueError: print('Download selesai') Ini adalah tampilan isi table dalam databasenya:","title":"Crawling"},{"location":"proses/#proses-web-crawling","text":"","title":"Proses Web Crawling"},{"location":"proses/#halaman-web","text":"Untuk website yang akan di crawl datanya yaitu: https://palominobag.com/best-seller Data yang ada pada website tersebut sebanyak 153 data.","title":"Halaman Web"},{"location":"proses/#proses-crawling","text":"Untuk mengakses halaman web dan tag html digunakan library: import requests from bs4 import BeautifulSoup Untuk menghubungkan ke website dan mengambil tag html digunakan code berikut: page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') links = soup.findAll(class_='product-name') links = soup.findAll(class_='product-name') Code diatas digunakan untuk mendapatkan semua tag html pada halaman web tersebut. Dan hasinya berupa list. Menentukan informasi apa saja yang akan diambil pada halaman web. Kemudian kita mengambil data berdasarkan class pada code html di website tersebut. Disini kita mengambil data yang berada pada class='product-name': for i in links: try : url = i.find ('a')['href'] print(url) page = requests.get(url) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find(class_='grid_12 alpha omega') judul = konten.find(class_='product-name no-rel').getText() harga = konten.find(class_='price').getText() detil = konten.find(class_='box-collateral').getText(' ') Untuk mengambil url yang ada pada class='product-name' digunakan: url = i.find ('a')['href'] print(url) Untuk mengambil data judul, harga, dan detil barang yang berada class='product-name' dan kemudian pencarian dipersempit lagi di class='grid_12 alpha omega' menggunakan: konten = soup.find(class_='grid_12 alpha omega') judul = konten.find(class_='product-name no-rel').getText() harga = konten.find(class_='price').getText() detil = konten.find(class_='box-collateral').getText(' ') Variable konten diatas menyimpan setiap data yang diambil pada class='grid_12 alpha omega' kemudian variable judul, harga,detil akan mengambil data yang telah ditampung didalam variable konten Untuk mengambil sebanyak 153 data yang ada pada website tesebut menggunakan: url=[str(i) for i in range(0,5)] for a in url: src= ('https://palominobag.com/best-seller?limit=36&p='+a) crawl (src) conn.commit()","title":"Proses Crawling"},{"location":"proses/#membuat-dan-menyimpan-data-di-database","text":"Untuk membuat database dan tablenya menggunakan code dibawah ini: conn = sqlite3.connect('ba.db') c = 1 choice = input(\"Update data? Y/N\").lower() if choice == 'y': conn.execute('drop table if exists BOOK') conn.execute('''CREATE TABLE BOOK (JUDUL TEXT NOT NULL, HARGA TEXT NOT NULL, DESKRIPSI TEXT NOT NULL);''') Kemudian data yang telah di crawl diatas disimpan kedalam table database yang telah dibuat menggunakan code dibawah ini: conn.execute(\"INSERT INTO BOOK \\ VALUES (?, ?, ?)\", (judul, harga, detil)); except AttributeError: print(\"\") conn.commit() except ValueError: print('Download selesai') Ini adalah tampilan isi table dalam databasenya:","title":"Membuat dan Menyimpan Data di Database"},{"location":"py/","text":"Code Lengkap Python \u00b6 Dibawah ini merupakan code lengkap crawler data import requests from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score import numpy as np import skfuzzy as fuzz def crawl(src): global c try : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') links = soup.findAll(class_='product-name') for i in links: try : url = i.find ('a')['href'] print(url) #src = each_link['href'] page = requests.get(url) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find(class_='grid_12 alpha omega') judul = konten.find(class_='product-name no-rel').getText() harga = konten.find(class_='price').getText() detil = konten.find(class_='box-collateral').getText(' ') conn.execute(\"INSERT INTO BOOK \\ VALUES (?, ?, ?)\", (judul, harga, detil)); except AttributeError: print(\"\") conn.commit() except ValueError: print('Download selesai') def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) def preprosesing(txt): SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() hasil = '' for i in txt.split(): if i[:-2].isalpha(): # Menghilangkan Kata tidak penting stop = stopword.remove(i) stem = stemmer.stem(stop) hasil += stem + ' ' return hasil #VSM def tokenisasi (txt,ngram=1): token=[] start=0 end=ngram txtSplit=txt.split() while end <=len (txtSplit): tmp=txtSplit[start:end] frase='' for i in tmp: frase += i+' ' token.append(frase) end+=1;start+=1; return token def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar,data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data,kataBaru conn = sqlite3.connect('ba.db') c = 1 choice = input(\"Update data? Y/N\").lower() if choice == 'y': conn.execute('drop table if exists BOOK') conn.execute('''CREATE TABLE BOOK (JUDUL TEXT NOT NULL, HARGA TEXT NOT NULL, DESKRIPSI TEXT NOT NULL);''') url=[str(i) for i in range(0,5)] for a in url: src= ('https://palominobag.com/best-seller?limit=36&p='+a) crawl (src) conn.commit() print(\"Building VSM...\") cursor = conn.execute(\"SELECT * from BOOK\") cursor = cursor.fetchall() #cursor = cursor[:10] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[2] token = tokenisasi(txt,1) cleaned=' ' for i in token: cek= preprosesing(i) if cek !=None: cleaned+=cek+' ' cleaned = cleaned[:-1] corpus.append(cleaned) d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) write_csv(\"bow_manual.csv\", VSM) vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1)) BoW_matrix = vectorizer.fit_transform(corpus) write_csv(\"bow_lib1.csv\", [vectorizer.get_feature_names()]) write_csv(\"bow_lib1.csv\", BoW_matrix.toarray()) #TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() write_csv(\"tfidf1.csv\", [feature_name]) write_csv(\"tfidf1.csv\", tfidf_matrix.toarray(), 'a') #Seleksi Fitur batas=0.8 fiturBaru,katabaru = seleksiFiturPearson(feature_name,tfidf_matrix.toarray(), batas) write_csv(\"Seleksi_Fitur.csv\",[katabaru]) write_csv(\"Seleksi_Fitur.csv\",fiturBaru,'a') #Cluster kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(tfidf_matrix, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i]))","title":"Code Lengkap"},{"location":"py/#code-lengkap-python","text":"Dibawah ini merupakan code lengkap crawler data import requests from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score import numpy as np import skfuzzy as fuzz def crawl(src): global c try : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') links = soup.findAll(class_='product-name') for i in links: try : url = i.find ('a')['href'] print(url) #src = each_link['href'] page = requests.get(url) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find(class_='grid_12 alpha omega') judul = konten.find(class_='product-name no-rel').getText() harga = konten.find(class_='price').getText() detil = konten.find(class_='box-collateral').getText(' ') conn.execute(\"INSERT INTO BOOK \\ VALUES (?, ?, ?)\", (judul, harga, detil)); except AttributeError: print(\"\") conn.commit() except ValueError: print('Download selesai') def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) def preprosesing(txt): SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() hasil = '' for i in txt.split(): if i[:-2].isalpha(): # Menghilangkan Kata tidak penting stop = stopword.remove(i) stem = stemmer.stem(stop) hasil += stem + ' ' return hasil #VSM def tokenisasi (txt,ngram=1): token=[] start=0 end=ngram txtSplit=txt.split() while end <=len (txtSplit): tmp=txtSplit[start:end] frase='' for i in tmp: frase += i+' ' token.append(frase) end+=1;start+=1; return token def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar,data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data,kataBaru conn = sqlite3.connect('ba.db') c = 1 choice = input(\"Update data? Y/N\").lower() if choice == 'y': conn.execute('drop table if exists BOOK') conn.execute('''CREATE TABLE BOOK (JUDUL TEXT NOT NULL, HARGA TEXT NOT NULL, DESKRIPSI TEXT NOT NULL);''') url=[str(i) for i in range(0,5)] for a in url: src= ('https://palominobag.com/best-seller?limit=36&p='+a) crawl (src) conn.commit() print(\"Building VSM...\") cursor = conn.execute(\"SELECT * from BOOK\") cursor = cursor.fetchall() #cursor = cursor[:10] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[2] token = tokenisasi(txt,1) cleaned=' ' for i in token: cek= preprosesing(i) if cek !=None: cleaned+=cek+' ' cleaned = cleaned[:-1] corpus.append(cleaned) d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) write_csv(\"bow_manual.csv\", VSM) vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1)) BoW_matrix = vectorizer.fit_transform(corpus) write_csv(\"bow_lib1.csv\", [vectorizer.get_feature_names()]) write_csv(\"bow_lib1.csv\", BoW_matrix.toarray()) #TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() write_csv(\"tfidf1.csv\", [feature_name]) write_csv(\"tfidf1.csv\", tfidf_matrix.toarray(), 'a') #Seleksi Fitur batas=0.8 fiturBaru,katabaru = seleksiFiturPearson(feature_name,tfidf_matrix.toarray(), batas) write_csv(\"Seleksi_Fitur.csv\",[katabaru]) write_csv(\"Seleksi_Fitur.csv\",fiturBaru,'a') #Cluster kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(tfidf_matrix, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i]))","title":"Code Lengkap Python"},{"location":"ref/","text":"Kesimpulan dan Referensi \u00b6 Kesimpulan \u00b6 Setelah mengambil 153 data pada website https://palominobag.com/best-seller kita mendapatkan data-data yang dapat dihitung jumlah katanya, dinilai tingkat kemiripan setiap datanya, dan dimasukkan dalam cluster yang sama jika tingkat kemiripannya sama. Disini vsm, tfidf, seleksi fitur, dan clustering memiliki pengaruh untuk menentukan itu semua. Untuk melihat code lengkapnya bisa click disini Referensi \u00b6 https://pintasku.com/tutorial/mengenal-dan-memahami-apa-itu-web-crawler/ https://informatikalogi.com/text-preprocessing/ https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/ https://medium.com/@rokamudrik/membuat-sebuah-chatbot-sederhana-yang-memiliki-fitur-reminder-bc2701e9d246 https://informatikalogi.com/algoritma-k-means-clustering/ https://jarvispatrick.wordpress.com/ https://smartstat.wordpress.com/2010/11/21/korelasi-pearson/","title":"Kesimpulan dan Referensi"},{"location":"ref/#kesimpulan-dan-referensi","text":"","title":"Kesimpulan dan Referensi"},{"location":"ref/#kesimpulan","text":"Setelah mengambil 153 data pada website https://palominobag.com/best-seller kita mendapatkan data-data yang dapat dihitung jumlah katanya, dinilai tingkat kemiripan setiap datanya, dan dimasukkan dalam cluster yang sama jika tingkat kemiripannya sama. Disini vsm, tfidf, seleksi fitur, dan clustering memiliki pengaruh untuk menentukan itu semua. Untuk melihat code lengkapnya bisa click disini","title":"Kesimpulan"},{"location":"ref/#referensi","text":"https://pintasku.com/tutorial/mengenal-dan-memahami-apa-itu-web-crawler/ https://informatikalogi.com/text-preprocessing/ https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/ https://medium.com/@rokamudrik/membuat-sebuah-chatbot-sederhana-yang-memiliki-fitur-reminder-bc2701e9d246 https://informatikalogi.com/algoritma-k-means-clustering/ https://jarvispatrick.wordpress.com/ https://smartstat.wordpress.com/2010/11/21/korelasi-pearson/","title":"Referensi"},{"location":"seleksi/","text":"Seleksi Fitur \u00b6 Pengertian \u00b6 Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi. Pearson Correlation \u00b6 Pearson Correlation digunakan untuk mengetahui ada tidaknya hubungan antara 2 variabel, yaitu variabel bebas dan variabel tergantung yang berskala interval atau rasio (parametrik). Pearson Correlation hanya mengukur kekuatan hubungan linier dan tidak pada hubungan non linier. Harus diingat pula bahwa adanya hubungan linier yang kuat di antara variabel tidak selalu berarti ada hubungan kausalitas, sebab-akibat. Semakin tinggi nilai korelasinya maka semakin kuat hubungan antara fiturnya, tetapi jika semakin kecil nilai korelasinya maka semakin lemah hubungan antara fiturnya. Jika nilai korelasi kecil maka akan langsung di buang. Dibawah ini adalah code untuk menyeleksi fitur dengan menggunakan Pearson Corelation: def pearsonCalculate(data, u,v): atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar,data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data,kataBaru Data dari Seleksi Fitur akan disimpan pada file CSV setelah di seleksi, kemudian hasil yang akan ditampilan yaitu kurang dari batas. batas=0.8 fiturBaru,katabaru = seleksiFiturPearson(feature_name,tfidf_matrix.toarray(), batas) write_csv(\"Seleksi_Fitur.csv\",[katabaru]) write_csv(\"Seleksi_Fitur.csv\",fiturBaru,'a') Ini adalah tampilan pada file csv:","title":"Seleksi Fitur"},{"location":"seleksi/#seleksi-fitur","text":"","title":"Seleksi Fitur"},{"location":"seleksi/#pengertian","text":"Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi.","title":"Pengertian"},{"location":"seleksi/#pearson-correlation","text":"Pearson Correlation digunakan untuk mengetahui ada tidaknya hubungan antara 2 variabel, yaitu variabel bebas dan variabel tergantung yang berskala interval atau rasio (parametrik). Pearson Correlation hanya mengukur kekuatan hubungan linier dan tidak pada hubungan non linier. Harus diingat pula bahwa adanya hubungan linier yang kuat di antara variabel tidak selalu berarti ada hubungan kausalitas, sebab-akibat. Semakin tinggi nilai korelasinya maka semakin kuat hubungan antara fiturnya, tetapi jika semakin kecil nilai korelasinya maka semakin lemah hubungan antara fiturnya. Jika nilai korelasi kecil maka akan langsung di buang. Dibawah ini adalah code untuk menyeleksi fitur dengan menggunakan Pearson Corelation: def pearsonCalculate(data, u,v): atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar,data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data,kataBaru Data dari Seleksi Fitur akan disimpan pada file CSV setelah di seleksi, kemudian hasil yang akan ditampilan yaitu kurang dari batas. batas=0.8 fiturBaru,katabaru = seleksiFiturPearson(feature_name,tfidf_matrix.toarray(), batas) write_csv(\"Seleksi_Fitur.csv\",[katabaru]) write_csv(\"Seleksi_Fitur.csv\",fiturBaru,'a') Ini adalah tampilan pada file csv:","title":"Pearson Correlation"},{"location":"tfidf/","text":"Term Frequency dan Invers Document Frequency (TF-IDF) \u00b6 Term Frequency (TF) \u00b6 TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. Inverse Document Frequency (IDF) \u00b6 IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() write_csv(\"tfidf1.csv\", [feature_name]) write_csv(\"tfidf1.csv\", tfidf_matrix.toarray(), 'a') TfidfVectorizer() merupakan modul untuk vektorisasi dokumen dengan skor TF-IDF. Di belakang layar, TfidfVectorizer menggunakan estimator CountVectorizer yang gunakan untuk menghasilkan pengkodean bag-of-words untuk menghitung kemunculan token, diikuti oleh TfidfTransformer, yang menormalkan jumlah kemunculan ini dengan idf. Ini adalah tampilan dari file csv:","title":"TF-IDF"},{"location":"tfidf/#term-frequency-dan-invers-document-frequency-tf-idf","text":"","title":"Term Frequency dan Invers Document Frequency (TF-IDF)"},{"location":"tfidf/#term-frequency-tf","text":"TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar.","title":"Term Frequency (TF)"},{"location":"tfidf/#inverse-document-frequency-idf","text":"IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() write_csv(\"tfidf1.csv\", [feature_name]) write_csv(\"tfidf1.csv\", tfidf_matrix.toarray(), 'a') TfidfVectorizer() merupakan modul untuk vektorisasi dokumen dengan skor TF-IDF. Di belakang layar, TfidfVectorizer menggunakan estimator CountVectorizer yang gunakan untuk menghasilkan pengkodean bag-of-words untuk menghitung kemunculan token, diikuti oleh TfidfTransformer, yang menormalkan jumlah kemunculan ini dengan idf. Ini adalah tampilan dari file csv:","title":"Inverse Document Frequency (IDF)"},{"location":"vsm/","text":"Vector Space Model (VSM) \u00b6 Vector Space Model (VSM) merupakan model aljabar yang merepresentasikan kumpulan dokumen sebagai vetctor. VSM dapat diaplikasikan dalam klasifikasi dokumen, clustering dokumen, dan scoring dokumen terhadap sebuah query. Dalam VSM setiap dokumen direpresentasikan sebagai sebuah vector, dimana nilai dari setiap nilai dari vector tersebut mewakili weight sebuah term. def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d Code diatas digunakan untuk menghitung banyaknya data yang ada pada dokumen. def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) Code diatas digunakan untuk membuat matrix VSM Code yang sudah dibuat diatas akan dipanggil dalam proses dibawah ini. cursor = cursor.fetchall() cursor = cursor[:50] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[2] token = tokenisasi(txt,1) cleaned=' ' for i in token: cek= preprosesing(i) if cek !=None: cleaned+=cek+' ' cleaned = cleaned[:-1] corpus.append(cleaned) d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) Jika telah selesai kita dapat memasukkan data VSM kedalam CSV agar dapat dibaca dan rapi dengan code: write_csv(\"bow_manual.csv\", VSM) Ini merupakan hasil tampilan pada file csv","title":"Vector Space Model"},{"location":"vsm/#vector-space-model-vsm","text":"Vector Space Model (VSM) merupakan model aljabar yang merepresentasikan kumpulan dokumen sebagai vetctor. VSM dapat diaplikasikan dalam klasifikasi dokumen, clustering dokumen, dan scoring dokumen terhadap sebuah query. Dalam VSM setiap dokumen direpresentasikan sebagai sebuah vector, dimana nilai dari setiap nilai dari vector tersebut mewakili weight sebuah term. def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d Code diatas digunakan untuk menghitung banyaknya data yang ada pada dokumen. def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) Code diatas digunakan untuk membuat matrix VSM Code yang sudah dibuat diatas akan dipanggil dalam proses dibawah ini. cursor = cursor.fetchall() cursor = cursor[:50] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[2] token = tokenisasi(txt,1) cleaned=' ' for i in token: cek= preprosesing(i) if cek !=None: cleaned+=cek+' ' cleaned = cleaned[:-1] corpus.append(cleaned) d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) Jika telah selesai kita dapat memasukkan data VSM kedalam CSV agar dapat dibaca dan rapi dengan code: write_csv(\"bow_manual.csv\", VSM) Ini merupakan hasil tampilan pada file csv","title":"Vector Space Model (VSM)"}]}